ä»¥ä¸‹æ˜¯æ ¹æ®ä½ çš„è¦æ±‚æ’°å†™çš„ GitHub é¡¹ç›® README è‰ç¨¿ï¼Œä½ å¯ä»¥æ ¹æ®é¡¹ç›®åç§°å’Œå…·ä½“æ–‡ä»¶è·¯å¾„è¿›ä¸€æ­¥è¡¥å……ç»†èŠ‚ï¼š

---

# Multi-Agent RLCard-PyMARL Integration for Card Games

This project bridges [RLCard](https://github.com/datamllab/rlcard) and [PyMARL](https://github.com/oxwhirl/pymarl), providing a flexible and efficient multi-agent reinforcement learning framework tailored for card games such as **Dou Di Zhu (æ–—åœ°ä¸»)**.

## ğŸ” Project Highlights

### âœ… Integration of RLCard and PyMARL

* Seamlessly combines **RLCard**, a toolkit for card game environments, with **PyMARL**, a popular multi-agent RL framework.
* Enables efficient multi-agent training for complex environments like Dou Di Zhu.

### âš™ï¸ Simplified Configuration

* Easy setup of **algorithm**, **hyperparameters**, and **environment** through intuitive configuration files.
* Supports standard PyMARL algorithms.

### ğŸ‘¥ Unified Model for Multiple Roles

* Trains **one single model** to control multiple roles (e.g., **landlord** and **farmers** in Dou Di Zhu), instead of designing separate models like [DouZero](https://github.com/kwai/DouZero).
* This results in **fewer parameters**, **faster training**, and **simpler deployment**.

### ğŸš€ Efficient Training with Competitive Performance

Our model achieves **\~84% win rate** against RLCard's rule-based AI after just **0.6million games** (roughly **1 day** of training), while DouZero reports **\~90%** with **200 million games over 30 days**.
Despite using only **\~1.5% of DouZero's training samples**, we reach comparable performance â€” demonstrating remarkable **training efficiency**.

All training is done on **consumer hardware**:
ğŸ’» **RTX 4070 Ti Super** + ğŸ§  **Intel i5-14600KF** on Ubuntu 22.04,
making our approach much more **accessible and lightweight** than previous methods.

---

### ğŸ§Š Self-Play with Rule-Based AI Warm-Start

* Supports **cold-start training** by using **rule-based agents** from RLCard as opponents in early self-play phases, enabling more stable convergence.
### ğŸ§  Full-Episode Training vs. DMC

* Unlike DouZeroâ€™s **Deep Monte Carlo (DMC)** approach, which estimates Q-values based on individual decisions, we **train using full game episodes**, enabling **credit assignment across the entire trajectory** and better coordination between agents.

### ğŸ“Š Training Visualization via Weights & Biases (WandB)


### ğŸ“Š Training Visualization via Weights & Biases (WandB)

* Tracks training metrics in real time using [WandB](https://wandb.ai/), fully integrated via PyMARLâ€™s logging backend.

---

## ğŸ“¦ Installation

```bash
# Clone this repository
git clone https://github.com/LaZuiYa/pymarl-rlcard.git
cd pymarl-rlcard

# Install dependencies (make sure to install RLCard and PyMARL as well)
pip install -r requirements.txt
```

You may need to manually install:

```bash
pip install rlcard wandb
```

---


## ğŸ“¬ Contact

For any questions or collaboration inquiries, please reach out to:
ğŸ“§ **[1653649769@hrbeu.edu.cn](mailto:1653649769@hrbeu.edu.cn)**

---

## â­ If You Like This Work

Please **star** this repository and **cite** our work if it helps your research or application. Your support motivates further development!

---

éœ€è¦æˆ‘å¸®ä½ åŠ ä¸Š badgeã€ç»“æ„ç›®å½•æˆ–å…·ä½“å‘½ä»¤è¿è¡Œç¤ºä¾‹ç­‰å†…å®¹å—ï¼Ÿ
